{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e174a1-41c9-4ff2-89d2-596e4232481c",
   "metadata": {},
   "source": [
    "# Day 2: RAG\n",
    "\n",
    "### What is RAG? \n",
    "\n",
    "**RAG or Retrieval Augmented Generation** - is a technique that is widely used in industry to help reduce model hallucination.\n",
    "For example, have you ever tryed to ask ChatGPT as questions about something that you know about and the answer is just made up?\n",
    "This is what we call a **hallucination** and RAG is a technique that we use to reduce hallucination. \n",
    "\n",
    "RAG does this by finding relavant chuncks of text to add to your prompt before sending it off to a LLM.\n",
    "\n",
    "**Why is this useful?** \n",
    "Models like ChatGPT are training on the internet. What if you ask it a questions, which it wasn't training on? Can you think of such questions? \n",
    "For example, let's say you have a business with your own data. You don't share any of that data on the internet or to anyone outside your business. \n",
    "How can you create a chatbot that let's you have a converation with your business data? You use RAG!\n",
    "\n",
    "Today we will learn about how you process document, put them into a database and how you retrieve those documents from a database. Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e2bdd23-7daf-4401-957e-b00ed6100b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader                    # special method to load PDF files\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader         # special method to load HTML\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader     # special method to load Markdown\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader                # helps use load multiple documents from a directory\n",
    "from langchain_community.document_loaders.merge import MergedDataLoader         # helps merge multiple different types of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962c08b-43db-4a4f-9ccf-d23dcc0ae021",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Loading Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f8ec4-d87c-4fc5-bdf4-b198ef885957",
   "metadata": {},
   "source": [
    "## 📃 Step 1: Simple loading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005edfd8-1915-4d2d-966e-379d972d08ce",
   "metadata": {},
   "source": [
    "### PDF document loading\n",
    "\n",
    "+ [Loading PDF files reference](https://python.langchain.com/docs/how_to/document_loader_pdf/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a18f6b6b-759c-4987-b264-d39f6095b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set the file path for the documents you want to load\n",
    "file_path_pdf = \"../data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf\"\n",
    "\n",
    "# Step 2: Create loader that is specific for your document type\n",
    "loader_pdf = PyPDFLoader(file_path_pdf)\n",
    "\n",
    "# Step 3: Load in the document\n",
    "my_pdf_document = loader_pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52e5942a-b413-4e1e-94b1-40a33200d68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf', 'page': 0}, page_content='Principles For Navigating\\nBIG DEBT CRISES\\nPart 1:\\nThe Archetypal Big Debt Cycle')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Examine what we just did\n",
    "#         'my_document' - it contains metadata, source, page number and the content of the page we just \"scrapped\"\n",
    "\n",
    "my_pdf_document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d84a76-ed61-4ada-b482-7692b37a72eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:        ../data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf\n",
      "Page number:   0\n",
      "Page content:  Principles For Navigating\n",
      "BIG DEBT CRISES\n",
      "Part 1:\n",
      "The Archetypal Big Debt Cycle\n"
     ]
    }
   ],
   "source": [
    "print(\"Source:       \", my_pdf_document[0].metadata[\"source\"])\n",
    "print(\"Page number:  \",   my_pdf_document[0].metadata[\"page\"])\n",
    "print(\"Page content: \", my_pdf_document[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5cb34-0ad7-4876-820a-dbfaeb193166",
   "metadata": {},
   "source": [
    "### HTML document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdace9ad-3050-446c-a621-ceb678d65b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setting the file path\n",
    "file_path_html = \"../data/Is_It_Wrong_to_Remove_a_Card_From_Monopoly_The_New_York_Times.html\"\n",
    "\n",
    "# Step 2: Create loader that is specific for your document type\n",
    "loader_html = UnstructuredHTMLLoader(file_path_html)\n",
    "\n",
    "# Step 3: Load in the document\n",
    "my_html_document = loader_html.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34353e77-28f3-4e41-bb38-6a864f3bc2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:        ../data/Is_It_Wrong_to_Remove_a_Card_From_Monopoly_The_New_York_Times.html\n",
      "Page content:  Magazine|Is It Wrong to Remove a Card From Monopoly?\n",
      "\n",
      "https://www.nytimes.com/2025/04/16/magazine/mo\n"
     ]
    }
   ],
   "source": [
    "print(\"Source:       \", my_html_document[0].metadata[\"source\"])\n",
    "print(\"Page content: \", my_html_document[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c59ec-e291-40f7-a93b-70c955edde0e",
   "metadata": {},
   "source": [
    "### Markdown document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ee689a-eb41-48cb-aa2a-7989361ab800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setting the file path\n",
    "file_path_markdown = \"../data/README.md\"\n",
    "\n",
    "# Step 2: Create loader that is specific for your document type\n",
    "loader_markdown = UnstructuredMarkdownLoader(file_path_markdown)\n",
    "\n",
    "# Step 3: Load in the document\n",
    "my_markdown_document = loader_markdown.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dec293ee-4c8a-4d9a-ab9b-37955f51c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/README.md'}, page_content='Release Notes\\n\\nCI\\n\\nPyPI - License\\n\\nPyPI - Downloads\\n\\nGitHub star chart\\n\\nOpen Issues\\n\\nOpen in Dev Containers\\n\\n\\n\\nTwitter\\n\\nCodSpeed Badge\\n\\n[!NOTE] Looking for the JS/TS library? Check out LangChain.js.\\n\\nLangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.\\n\\nbash pip install -U langchain\\n\\nTo learn more about LangChain, check out the docs. If you’re looking for more advanced customization or agent orchestration, check out LangGraph, our framework for building controllable agent workflows.\\n\\nWhy use LangChain?\\n\\nLangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.\\n\\nUse LangChain for: - Real-time data augmentation. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more. - Model interoperability. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.\\n\\nLangChain’s ecosystem\\n\\nWhile the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.\\n\\nTo improve your LLM application development, pair LangChain with:\\n\\nLangSmith - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\n\\nLangGraph - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.\\n\\nLangGraph Platform - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in LangGraph Studio.\\n\\nAdditional resources\\n\\nTutorials: Simple walkthroughs with guided examples on getting started with LangChain.\\n\\nHow-to Guides: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.\\n\\nConceptual Guides: Explanations of key concepts behind the LangChain framework.\\n\\nAPI Reference: Detailed reference on navigating base packages and integrations for LangChain.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_markdown_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7571da7-f411-445e-8b5d-7506955ecbbd",
   "metadata": {},
   "source": [
    "## 🦜 Langchain provide ways of loading in all types of documents \n",
    "\n",
    "Here is a reference for loading documents that I didn't cover in the examples. When we begin to work on our group project these links can be useful to help you load all types of document types.\n",
    "\n",
    "#### Reference: [Document loaders](https://python.langchain.com/docs/how_to/#document-loaders)\n",
    "+ [How to: load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n",
    "+ [How to: load web pages](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "+ [How to: load CSV data](https://python.langchain.com/docs/how_to/document_loader_csv/)\n",
    "+ [How to: load HTML data](https://python.langchain.com/docs/how_to/document_loader_html/)\n",
    "+ [How to: load JSON data](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "+ [How to: load Markdown data](https://python.langchain.com/docs/how_to/document_loader_markdown/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb36e8-2608-42c7-a102-05909e27524b",
   "metadata": {},
   "source": [
    "## 📂 Step 2: Loading documents from a folder\n",
    "\n",
    "**Scenario:** What if you have a folder full of documents and you want to load in all of the PDF documents you have at the same time?\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/image1.png\" width=\"25%\" height=\"25%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6bff0-6687-48f4-b3ad-921639d3609c",
   "metadata": {},
   "source": [
    "### 2.1: Let's say we want to load in all of the PDF files we have in a folder into a document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "238f750a-1bba-44f4-8407-3cec785b7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Using Langchain's DirectoryLoader set the path to your folder and use\n",
    "pdfs_file_path = \"../data/\"\n",
    "\n",
    "# Step 2: use Directory loader to load in all of the documents from a folder\n",
    "\n",
    "loader_pdfs = DirectoryLoader(pdfs_file_path,                # set your base file path\n",
    "                              glob=\"*.pdf\",                  # *.pdf means all files with the extension .pdf \n",
    "                              show_progress=True,            # progress bar\n",
    "                              use_multithreading=True,       # load from disk using multiple threads\n",
    "                              loader_cls=PyPDFLoader         # add the langchain PDF class\n",
    "                             )\n",
    "\n",
    "# Step 3: Load in the documents\n",
    "my_pdf_documents = loader_pdfs.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28243f5e-dae9-4608-9101-a766cc36c09a",
   "metadata": {},
   "source": [
    "#### Let's talk about what just happend.\n",
    "\n",
    "In this project - there is a folder called `data/`. That folder contains three different documents that end in a .pdf.\n",
    "    \n",
    "+ data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf\n",
    "+ data/react.pdf\n",
    "+ data/trends_in_deep_learning_nlp.pdf\n",
    "\n",
    "`DirectoryLoader` took all of the documents ending in .pdf, saved them to a list of `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55f3c384-09bc-4164-84bf-18353128b0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../data/react.pdf', 'page': 0}, page_content='Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd343898-c4b2-4a8a-a675-1d4e02cabbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source of the text:   ../data/react.pdf\n",
      "Page:                 0\n"
     ]
    }
   ],
   "source": [
    "# We can also look at the details of the Document object\n",
    "print(\"Source of the text:  \" , my_pdf_documents[0].metadata[\"source\"])\n",
    "print(\"Page:                \" , my_pdf_documents[0].metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831da732-47c0-4632-a855-4ae1b6a31704",
   "metadata": {},
   "source": [
    "#### What if you have a scenario where you want to add more data to the `Document` object? \n",
    "\n",
    "+ For example - your data is missing the authors name, but you know the author of the document and want to add it to use if for a later task - for example, as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc83d397-f075-4e15-b927-cbc86d867027",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pdf_documents[0].metadata[\"author\"] = \"Andronik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7ea33f9-5713-486a-b49a-d3cd52a921ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andronik'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf_documents[0].metadata[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7eb7c97-bab4-407f-b96f-310a47a8db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '../data/react.pdf', 'page': 0, 'author': 'Andronik'}, page_content='Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100f7ac-bb58-408e-b4f9-62ca0406fa03",
   "metadata": {},
   "source": [
    "### 2.2: Let's do something similar but with HTML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2a9c17f0-09d7-4652-8ba2-af4161aa8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Using Langchain's DirectoryLoader set the path to your folder and use\n",
    "html_file_path = \"../data/\"\n",
    "\n",
    "# Step 2: use Directory loader to load in all of the documents from a folder\n",
    "\n",
    "loader_html = DirectoryLoader(html_file_path,                       \n",
    "                              glob=\"*.html\",\n",
    "                              show_progress=True,\n",
    "                              use_multithreading=True,\n",
    "                              loader_cls=UnstructuredHTMLLoader              # the only thing that changes from the top example\n",
    "                             )\n",
    "\n",
    "# Step 3: Load in the documents\n",
    "my_html_documents = loader_html.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "39468d5c-ceb8-4840-b3f2-0f819b6ec2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source of the text:   ../data/Is_It_Wrong_to_Remove_a_Card_From_Monopoly_The_New_York_Times.html\n"
     ]
    }
   ],
   "source": [
    "# Same applies as above example\n",
    "print(\"Source of the text:  \" , my_html_documents[0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2a0efb9a-d047-4beb-a771-29aa2217510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Magazine|Is It Wrong to Remove a Card From Monopoly?\\n\\nhttps://www.nytimes.com/2025/04/16/magazine/monopoly-games-children-ethics.html\\n\\nAdvertisement\\n\\nSKIP ADVERTISEMENT\\n\\nSubscriber-only Newsletter\\n\\nThe Ethicist\\n\\nIs It Wrong to Remove a Card From Monopoly?\\n\\nThe magazine’s Ethicist columnist on altering board games to teach children ethical behavior.\\n\\nKwame Anthony Appiah\\n\\nBy Kwame Anthony Appiah\\n\\nApril 16, 2025\\n\\nYou’re reading The Ethicist newsletter, for Times subscribers only. Advice on life’s trickiest situations and moral dilemmas from the philosopher Kwame Anthony Appiah.\\n\\nMy grandchildren love playing Monopoly. The board game has become a great way for me to interact with them, and also a great way for them to see capitalism in all its imperfect glory. The problem: One of the cards a player may draw when landing on Community Chest is “Bank Error in Your Favor. Collect $200.” Right when we first started playing the game together, I removed that card from the set. I did so because i'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_html_documents[0].page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdf4db-52d0-4055-b73e-55e7f12ff6f8",
   "metadata": {},
   "source": [
    "## 🗂️ Step 3: Loading different documents from the same folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0bf20d2c-b329-443a-b995-9d81f32d805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:12<00:00,  4.23s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  2.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 69.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Loading in the PDF, HTML and Markdown files\n",
    "file_path_data = \"../data/\"\n",
    "\n",
    "loader_pdfs = DirectoryLoader(file_path_data,              \n",
    "                              glob=\"*.pdf\",              \n",
    "                              show_progress=True,            \n",
    "                              use_multithreading=True,     \n",
    "                              loader_cls=PyPDFLoader)                   # on difference is the loader class\n",
    "\n",
    "\n",
    "loader_html = DirectoryLoader(file_path_data,                       \n",
    "                              glob=\"*.html\",\n",
    "                              show_progress=True,\n",
    "                              use_multithreading=True,\n",
    "                              loader_cls=UnstructuredHTMLLoader)\n",
    "\n",
    "\n",
    "loader_markdown = DirectoryLoader(file_path_data,                       \n",
    "                                  glob=\"*.md\",\n",
    "                                  show_progress=True,\n",
    "                                  use_multithreading=True,\n",
    "                                  loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: User MergeDataLoader to combine all of the different loader types you have for your data\n",
    "my_loaders = MergedDataLoader(loaders=[loader_pdfs, loader_html, loader_markdown])\n",
    "\n",
    "# Step 3: Load all documents into a list of Document objects\n",
    "my_docs = my_loaders.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d19d4fc0-dae3-490d-889e-f43794fd39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of my list of Documents:  548\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of my list of Documents: \", len(my_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9ff6330a-f8ca-4e93-a091-8bf0f2476ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n",
      "{'source': '../data/react.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 10:\n",
    "    \n",
    "    for doc in my_docs:\n",
    "        print(doc.metadata)\n",
    "        break\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2f3e85-34fc-4d1d-ba3f-62d90a0e5752",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: RAG\n",
    "\n",
    "What is RAG and why is it important? Let's say that you are working at a company and your boss want you to create a chatbot that is able to answer questions about your business. For example, how much inventory do we have left in the parts department? What are the type of defects we had for Part A in the month of March? \n",
    "\n",
    "To answer your bosses questions using a chatbot your chatbot needs to be up to date on the information your business has. Plus, the data your business has is not on the internet!\n",
    "\n",
    "RAG is a way of adding extra knowledge to the LLM without needing to retrain the entire model, which is expensive, time consuming and requires expertise. What RAG provides use is a database of additional information which we can hookup to the LLM and use to answer questions.\n",
    "\n",
    "Here is a diagram and next we will talk about how it works.\n",
    "\n",
    "![rag](../assets/rag.png)\n",
    "\n",
    "\n",
    "Three important parts to a RAG:\n",
    "1. **Document Splitting** - how we chop of the documents into smaller pieces.\n",
    "2. **Embeddings** - how we take those smaller pieces and put them into a database.\n",
    "3. **Retrieval** - how we retrieve pieces of our documents from the RAG\n",
    "\n",
    "\n",
    "If you want more details RAG. Here is a paper you can check out...\n",
    "+ [What is a RAG?](https://www.youtube.com/watch?v=T-D1OfcDW1M)\n",
    "+ [Searching for Best Practices in Retrieval-Augmented\n",
    "Generation](https://arxiv.org/pdf/2407.01219)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc9392-90bc-4565-a54a-c5251e22c574",
   "metadata": {},
   "source": [
    "## Step 1: Chunking Documents and Embeddings\n",
    "\n",
    "Once we know the documents that we want to include in our database, we need to processes the documents.\n",
    "**Chunking** is a way of splitting your documents into smaller pieces so when you need to retrieve something from your database you don't collect the entire document. You just collect a piece of it.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/text_splitting.png\" width=\"\" height=\"\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a135c0f-8cc3-4a0c-ab24-2087dcdc6b0e",
   "metadata": {},
   "source": [
    "### Step 1.1: Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c4eca09-c82d-4650-8607-608f28ee543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6b87dae1-b345-4665-a482-163c810dbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05ed5844-09e8-409e-bbdb-dcf9668ebaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Hello. My name is Andronik and I have a cat named Jinxy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "813eb57f-e0aa-45b1-b629-40b1086ae339",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4b50b23-b5d3-4504-bcd8-3a21c0139cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk length: 4, Text Split: Hell\n",
      "Chunk length: 4, Text Split: llo.\n",
      "Chunk length: 2, Text Split: My\n",
      "Chunk length: 3, Text Split: nam\n",
      "Chunk length: 3, Text Split: ame\n",
      "Chunk length: 2, Text Split: is\n",
      "Chunk length: 3, Text Split: And\n",
      "Chunk length: 4, Text Split: ndro\n",
      "Chunk length: 4, Text Split: roni\n",
      "Chunk length: 3, Text Split: nik\n",
      "Chunk length: 3, Text Split: and\n",
      "Chunk length: 1, Text Split: I\n",
      "Chunk length: 3, Text Split: hav\n",
      "Chunk length: 3, Text Split: ave\n",
      "Chunk length: 1, Text Split: a\n",
      "Chunk length: 3, Text Split: cat\n",
      "Chunk length: 3, Text Split: nam\n",
      "Chunk length: 4, Text Split: amed\n",
      "Chunk length: 3, Text Split: Jin\n",
      "Chunk length: 4, Text Split: inxy\n",
      "------------------------------------------------------------\n",
      "Chunk length: 6, Text Split: Hello.\n",
      "Chunk length: 7, Text Split: My name\n",
      "Chunk length: 2, Text Split: is\n",
      "Chunk length: 7, Text Split: Androni\n",
      "Chunk length: 5, Text Split: ronik\n",
      "Chunk length: 5, Text Split: and I\n",
      "Chunk length: 6, Text Split: I have\n",
      "Chunk length: 5, Text Split: a cat\n",
      "Chunk length: 5, Text Split: named\n",
      "Chunk length: 5, Text Split: Jinxy\n",
      "------------------------------------------------------------\n",
      "Chunk length: 14, Text Split: Hello. My name\n",
      "Chunk length: 10, Text Split: My name is\n",
      "Chunk length: 15, Text Split: is Andronik and\n",
      "Chunk length: 12, Text Split: and I have a\n",
      "Chunk length: 10, Text Split: have a cat\n",
      "Chunk length: 11, Text Split: a cat named\n",
      "Chunk length: 11, Text Split: named Jinxy\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in [4, 8, 16]:\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=n,\n",
    "    chunk_overlap=n//2)\n",
    "\n",
    "    text = text_splitter.split_text(test_sentence)\n",
    "\n",
    "    for t in text:\n",
    "        print(f\"Chunk length: {len(t)}, Text Split: {t}\")\n",
    "\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbde3fd-7ba8-4206-8489-082f83c9d610",
   "metadata": {},
   "source": [
    "### Step 1.2: Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5613934-5953-4ad4-81e6-6ca5aba82325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in the api keys to connect to OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "615df564-8aa2-410f-98df-f60501e024be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebea1c05-f153-429d-bbc8-8d1a61ebeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to OpenAI \n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_organization = os.environ.get(\"OPENAI_ORGANIZATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "640d7623-79b6-459d-891f-28205ebacd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,\n",
    "                          model=\"text-embeddings-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "686c0ac0-a2dd-43ec-aa48-fc5baceb62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0a38e629-5b72-49bf-a031-cec8a8696c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"\n",
    "This raises the question: Can function inlining affect the output of a numerical program? \n",
    "I’m interested in both the programmer inlining a function manually in his editor and the compiler doing it for you during an optimization pass.\n",
    "It turns out that yes, there are multiple ways in which inlining can change results, and specifics depend on the interplay of your language spec, \n",
    "compiler and hardware. One of the big reasons why compilers do inlining is that it increases the scope for optimizations. Most optimization passes \n",
    "run on individual functions, so inlining gives the compiler more code to work with, and more opportunities to apply potentially result-changing optimizations.\n",
    "Here’s a concrete example that compiled with gcc -O3 -march=haswell produces different results depending on whether the function is inlined or not\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e45f9e5-88ab-4216-b179-6d8b5fd41774",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_text(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0b34cfdd-7cd7-4884-996b-b301d61a230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "This raises the question: Can function inlining affect the output of a numerical program? I’m interested in both the programmer inlining a function manually in his editor and the compiler doing it for you during an optimization pass. It turns out that yes, there are multiple ways in which inlining can change results, and specifics depend on the interplay of your language spec, \n",
      "compiler and hardware. One of the big reasons why compilers do inlining is that it increases the scope for optimizations.\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "Most optimization passes \n",
      "run on individual functions, so inlining gives the compiler more code to work with, and more opportunities to apply potentially result-changing optimizations. Here’s a concrete example that compiled with gcc -O3 -march=haswell produces different results depending on whether the function is inlined or not\n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of chunks:\", len(docs))\n",
    "for doc in docs:\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(doc)\n",
    "    print(\"--------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906da9f5-7f4d-4e4f-a89f-5e167d4f2f6d",
   "metadata": {},
   "source": [
    "### What did the `OpenAIEmbeddings()` do?\n",
    "\n",
    "The basic idea is this...\n",
    "\n",
    "The embeddings model take a look at the text that we give it and decides to devide it into 2 pieces. Each piece we can think of as an \"idea\". `Chunk 1` represents idea 2 and `Chunk 2` represents idea 2.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "node1[Document]\n",
    "node2[\"Chunk 1\"]\n",
    "node3[\"Chunk 2\"]\n",
    "node1-->node2\n",
    "node1-->node3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f76b9-01ce-425b-ab3f-2ffd7409c609",
   "metadata": {},
   "source": [
    "## Step 2: Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74f849bd-8faa-4f53-9955-d5edc2bd30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b11b7650-f3e9-40fc-ba0a-deaf837d6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first initilize an embeddings model to use on your documents\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,\n",
    "                              model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8006dca-14b9-4b08-8768-587095ed2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Let's chunk the documents into smaller pieces\n",
    "text_splitter = SemanticChunker(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "1246d918-934f-4827-b0ce-f8be8df1d6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in my_docs 548\n",
      "-------------------------------------------\n",
      "\n",
      "page_content='Published as a conference paper at ICLR 2023\n",
      "REAC T: S YNERGIZING REASONING AND ACTING IN\n",
      "LANGUAGE MODELS\n",
      "Shunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n",
      "1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team\n",
      "1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n",
      "ABSTRACT\n",
      "While large language models (LLMs) have demonstrated impressive performance\n",
      "across tasks in language understanding and interactive decision making, their\n",
      "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\n",
      "plan generation) have primarily been studied as separate topics. In this paper, we\n",
      "explore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\n",
      "in an interleaved manner, allowing for greater synergy between the two: reasoning\n",
      "traces help the model induce, track, and update action plans as well as handle\n",
      "exceptions, while actions allow it to interface with and gather additional information\n",
      "from external sources such as knowledge bases or environments. We apply our\n",
      "approach, named ReAct, to a diverse set of language and decision making tasks\n",
      "and demonstrate its effectiveness over state-of-the-art baselines in addition to\n",
      "improved human interpretability and trustworthiness. Concretely, on question\n",
      "answering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\n",
      "issues of hallucination and error propagation in chain-of-thought reasoning by\n",
      "interacting with a simple Wikipedia API, and generating human-like task-solving\n",
      "trajectories that are more interpretable than baselines without reasoning traces.\n",
      "Furthermore, on two interactive decision making benchmarks (ALFWorld and\n",
      "WebShop), ReAct outperforms imitation and reinforcement learning methods by\n",
      "an absolute success rate of 34% and 10% respectively, while being prompted with\n",
      "only one or two in-context examples.\n",
      "1 I NTRODUCTION\n",
      "A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\n",
      "verbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\n",
      "play an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\n",
      "1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\n",
      "sider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\n",
      "reason in language in order to track progress (“now that everything is cut, I should heat up the pot of\n",
      "water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\n",
      "me use soy sauce and pepper instead”), and to realize when external information is needed (“how do\n",
      "I prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\n",
      "recipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\n",
      "dish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\n",
      "to learn new tasks quickly and perform robust decision making or reasoning, even under previously\n",
      "unseen circumstances or facing information uncertainties.\n",
      "Recent results have hinted at the possibility of combining verbal reasoning with interactive decision\n",
      "making in autonomous systems. On one hand, properly prompted large language models (LLMs)\n",
      "have demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\n",
      "∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\n",
      "1\n",
      "arXiv:2210.03629v3  [cs.CL]  10 Mar 2023' metadata={'source': '../data/react.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "# Step 1.1: Let's take a look at what out documents look like and see if we need to make them smaller\n",
    "print(\"Number of documents in my_docs\", len(my_docs))\n",
    "print(\"-------------------------------------------\\n\")\n",
    "for doc in my_docs:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "84bd0143-b91d-4db4-92e4-cbb46246b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_documents(my_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a19034bd-517d-4a61-ba4e-d254fe7f6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length after chunking: 1255\n"
     ]
    }
   ],
   "source": [
    "print(\"New Length after chunking:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ca245c8b-f451-4edd-a5e5-586a80b014b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma(\n",
    "    collection_name=\"tumo_2025\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"../db/my_first_vdb\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bf209ece-a989-4947-9f5a-338e54072bf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Requested 336718 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/langchain_core/vectorstores/base.py:287\u001b[0m, in \u001b[0;36madd_documents\u001b[0;34m(self, documents, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m    286\u001b[0m     texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 287\u001b[0m     metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_texts(texts, metadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_documents` and `add_texts` has not been implemented \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/langchain_chroma/vectorstores.py:527\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(texts)\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 527\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m metadatas:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;66;03m# fill metadatas with empty dicts if somebody\u001b[39;00m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;66;03m# did not specify metadata for all texts\u001b[39;00m\n\u001b[1;32m    531\u001b[0m     length_diff \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(texts) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(metadatas)\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:483\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[0;32m--> 483\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invocation_params\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    487\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/openai/resources/embeddings.py:124\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    118\u001b[0m         embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[1;32m    119\u001b[0m             base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    120\u001b[0m         )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': 'Requested 336718 tokens, max 300000 tokens per request', 'type': 'max_tokens_per_request', 'param': None, 'code': 'max_tokens_per_request'}}"
     ]
    }
   ],
   "source": [
    "vectorstore.add_documents(documents=docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570823e-6dcb-4d3d-8008-dd02913c6601",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tumo-2025-summer-workshop",
   "language": "python",
   "name": "tumo-2025-summer-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
