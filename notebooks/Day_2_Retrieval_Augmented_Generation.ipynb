{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9e174a1-41c9-4ff2-89d2-596e4232481c",
   "metadata": {},
   "source": [
    "# Day 2: RAG\n",
    "\n",
    "### What is RAG? \n",
    "\n",
    "**RAG or Retrieval Augmented Generation** - is a technique that is widely used in industry to help reduce model hallucination.\n",
    "For example, have you ever tried to ask a LLM a question about something that you know about and the answer is just made up?\n",
    "This is what we call a **hallucination** and RAG is a technique that we use to reduce hallucination. \n",
    "\n",
    "**RAG** does this by finding relavant chuncks of text to add to your prompt before sending it off to a LLM.\n",
    "\n",
    "**Why is this useful?** \n",
    "Models like ChatGPT are training on a wide variety of data from the internet. What if you ask it a questions, which it wasn't training on? Can you think of such questions? \n",
    "For example, let's say you have a business with your own data. You don't share any of that data on the internet or to anyone outside your business. \n",
    "How can you create a chatbot that let's you have a converation with your business data? You use RAG!\n",
    "\n",
    "Today we will learn about how you process documents, put them into a database and how you retrieve those documents from a database. Let's get started! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d074952f",
   "metadata": {},
   "source": [
    "\n",
    "### Choosing an Embedding Model\n",
    "\n",
    "+ Embedding models are tools that we use to turn our documents into numbers we can store in a database we can later retieve.\n",
    "\n",
    "| Model | Dim | License | Notes |\n",
    "|-------|-----|---------|-------|\n",
    "| `text-embedding-3-small` (OpenAI) | 1536 | Commercial | Great multilingual coverage, pay‑per‑call |\n",
    "| `BAAI/bge-small-en-v1.5` | 384 | Apache 2.0 | State‑of‑the‑art open model, can do *instruction‑tuned* search (`<emb>` prompt) |\n",
    "| `sentence-transformers/all-MiniLM-L6-v2` | 384 | Apache 2.0 | Lightweight; good default |\n",
    "\n",
    "**Rule of thumb**: Pick the smallest model that meets quality requirements; larger ≠ always better. Measure with *mean reciprocal rank* (MRR) or *precision@k* on a small evaluation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083bc214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rich in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (14.0.0)\n",
      "Requirement already satisfied: langchain-community in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (0.3.26)\n",
      "Collecting pypdf\n",
      "  Downloading pypdf-5.7.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from rich) (2.19.2)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.67)\n",
      "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (0.3.26)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (2.0.41)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (2.32.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (3.12.13)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (2.10.1)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (0.4.4)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-community) (2.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
      "Requirement already satisfied: anyio in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Downloading pypdf-5.7.0-py3-none-any.whl (305 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.7.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install rich langchain-community pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2bdd23-7daf-4401-957e-b00ed6100b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "from rich import inspect\n",
    "\n",
    "# import core langchain libraries\n",
    "from langchain_community.document_loaders import PyPDFLoader                    # special method to load PDF files\n",
    "from langchain_community.document_loaders import UnstructuredHTMLLoader         # special method to load HTML\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader     # special method to load Markdown\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader                # helps use load multiple documents from a directory\n",
    "from langchain_community.document_loaders.merge import MergedDataLoader         # helps merge multiple different types of documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962c08b-43db-4a4f-9ccf-d23dcc0ae021",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 1: Loading Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232f8ec4-d87c-4fc5-bdf4-b198ef885957",
   "metadata": {},
   "source": [
    "## 📃 Step 1: Simple loading documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005edfd8-1915-4d2d-966e-379d972d08ce",
   "metadata": {},
   "source": [
    "### PDF document loading\n",
    "\n",
    "+ [Loading PDF files reference](https://python.langchain.com/docs/how_to/document_loader_pdf/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f27ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (5.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a18f6b6b-759c-4987-b264-d39f6095b4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set the file path for the documents you want to load\n",
    "file_path_pdf = \"../data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf\"\n",
    "\n",
    "# Step 2: Create loader that is specific for your document type\n",
    "loader_pdf = PyPDFLoader(file_path_pdf)\n",
    "\n",
    "# Step 3: Load in the document\n",
    "my_pdf_document = loader_pdf.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52e5942a-b413-4e1e-94b1-40a33200d68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'Adobe PDF Library 15.0', 'creator': 'Adobe InDesign CC 13.1 (Macintosh)', 'creationdate': '2018-09-26T11:32:24-05:00', 'author': 'Ray Dalio', 'keywords': 'Debt Crises; Bridgewater Associates; Ray Dalio', 'moddate': '2018-10-04T16:16:31-04:00', 'subject': 'Principles for Navigating Big Debt Crises provides a unique template for how debt crises work and the principles for dealing with them well.', 'title': 'Principles for Navigating Big Debt Crises', 'trapped': '/Unknown', 'source': '../data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf', 'total_pages': 480, 'page': 0, 'page_label': '1'}, page_content='Principles For Navigating\\nBIG DEBT CRISES\\nPart 1:\\nThe Archetypal Big Debt Cycle')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: Examine what we just did\n",
    "#         'my_document' - it contains metadata, source, page number and the content of the page we just \"scrapped\"\n",
    "\n",
    "my_pdf_document[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61d84a76-ed61-4ada-b482-7692b37a72eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:        ../data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf\n",
      "Page number:   0\n",
      "Page content:  Principles For Navigating\n",
      "BIG DEBT CRISES\n",
      "Part 1:\n",
      "The Archetypal Big Debt Cycle\n"
     ]
    }
   ],
   "source": [
    "print(\"Source:       \",   my_pdf_document[0].metadata[\"source\"])\n",
    "print(\"Page number:  \",   my_pdf_document[0].metadata[\"page\"])\n",
    "print(\"Page content: \",   my_pdf_document[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a5cb34-0ad7-4876-820a-dbfaeb193166",
   "metadata": {},
   "source": [
    "### HTML document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f5353ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.18.1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting lxml (from unstructured)\n",
      "  Downloading lxml-6.0.0-cp313-cp313-macosx_10_13_universal2.whl.metadata (6.6 kB)\n",
      "Collecting nltk (from unstructured)\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured) (2.32.4)\n",
      "Collecting beautifulsoup4 (from unstructured)\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting emoji (from unstructured)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured)\n",
      "  Downloading python_iso639-2025.2.18-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting langdetect (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured) (2.3.1)\n",
      "Collecting rapidfuzz (from unstructured)\n",
      "  Downloading rapidfuzz-3.13.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured) (4.14.0)\n",
      "Collecting unstructured-client (from unstructured)\n",
      "  Downloading unstructured_client-0.37.2-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting wrapt (from unstructured)\n",
      "  Downloading wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting tqdm (from unstructured)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: psutil in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured) (7.0.0)\n",
      "Collecting python-oxmsg (from unstructured)\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured)\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->unstructured)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from dataclasses-json->unstructured) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from dataclasses-json->unstructured) (0.9.0)\n",
      "Requirement already satisfied: six>=1.9 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from html5lib->unstructured) (1.17.0)\n",
      "Collecting webencodings (from html5lib->unstructured)\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting click (from nltk->unstructured)\n",
      "  Downloading click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from nltk->unstructured)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk->unstructured)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting olefile (from python-oxmsg->unstructured)\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests->unstructured) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests->unstructured) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests->unstructured) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from requests->unstructured) (2025.6.15)\n",
      "Collecting aiofiles>=24.1.0 (from unstructured-client->unstructured)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting cryptography>=3.1 (from unstructured-client->unstructured)\n",
      "  Downloading cryptography-45.0.4-cp311-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (2.11.7)\n",
      "Requirement already satisfied: pypdf>=4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (5.7.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from unstructured-client->unstructured) (1.0.0)\n",
      "Collecting cffi>=1.14 (from cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Downloading cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: anyio in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from httpx>=0.27.0->unstructured-client->unstructured) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx>=0.27.0->unstructured-client->unstructured) (0.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->unstructured) (24.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from pydantic>=2.11.2->unstructured-client->unstructured) (0.4.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured) (1.1.0)\n",
      "Collecting pycparser (from cffi>=1.14->cryptography>=3.1->unstructured-client->unstructured)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/andronikmk/Documents/Tumo_2025_Summer_Workshop/.venv/lib/python3.13/site-packages (from anyio->httpx>=0.27.0->unstructured-client->unstructured) (1.3.1)\n",
      "Downloading unstructured-0.18.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Downloading lxml-6.0.0-cp313-cp313-macosx_10_13_universal2.whl (8.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2025.2.18-py3-none-any.whl (167 kB)\n",
      "Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp313-cp313-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading unstructured_client-0.37.2-py3-none-any.whl (210 kB)\n",
      "Downloading wrapt-1.17.2-cp313-cp313-macosx_11_0_arm64.whl (38 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading cryptography-45.0.4-cp311-abi3-macosx_10_9_universal2.whl (7.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Downloading click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading cffi-1.17.1-cp313-cp313-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993332 sha256=59cd81738ae47279243cc8f89fe5f08a40f299e31276b5842244c5963e4e428c\n",
      "  Stored in directory: /Users/andronikmk/Library/Caches/pip/wheels/eb/87/25/2dddf1c94e1786054e25022ec5530bfed52bad86d882999c48\n",
      "Successfully built langdetect\n",
      "Installing collected packages: webencodings, filetype, wrapt, tqdm, soupsieve, regex, rapidfuzz, python-magic, python-iso639, pycparser, olefile, lxml, langdetect, joblib, html5lib, emoji, click, chardet, backoff, aiofiles, python-oxmsg, nltk, cffi, beautifulsoup4, cryptography, unstructured-client, unstructured\n",
      "Successfully installed aiofiles-24.1.0 backoff-2.2.1 beautifulsoup4-4.13.4 cffi-1.17.1 chardet-5.2.0 click-8.2.1 cryptography-45.0.4 emoji-2.14.1 filetype-1.2.0 html5lib-1.1 joblib-1.5.1 langdetect-1.0.9 lxml-6.0.0 nltk-3.9.1 olefile-0.47 pycparser-2.22 python-iso639-2025.2.18 python-magic-0.4.27 python-oxmsg-0.0.2 rapidfuzz-3.13.0 regex-2024.11.6 soupsieve-2.7 tqdm-4.67.1 unstructured-0.18.1 unstructured-client-0.37.2 webencodings-0.5.1 wrapt-1.17.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdace9ad-3050-446c-a621-ceb678d65b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setting the file path\n",
    "file_path_html = \"../data/Is_It_Wrong_to_Remove_a_Card_From_Monopoly_The_New_York_Times.html\"\n",
    "\n",
    "# Step 2: Create loader that is specific for your document type\n",
    "loader_html = UnstructuredHTMLLoader(file_path_html)\n",
    "\n",
    "# Step 3: Load in the document\n",
    "my_html_document = loader_html.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34353e77-28f3-4e41-bb38-6a864f3bc2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:        ../data/Is_It_Wrong_to_Remove_a_Card_From_Monopoly_The_New_York_Times.html\n",
      "Page content:  Magazine|Is It Wrong to Remove a Card From Monopoly?\n",
      "\n",
      "https://www.nytimes.com/2025/04/16/magazine/mo\n"
     ]
    }
   ],
   "source": [
    "print(\"Source:       \", my_html_document[0].metadata[\"source\"])\n",
    "print(\"Page content: \", my_html_document[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5c59ec-e291-40f7-a93b-70c955edde0e",
   "metadata": {},
   "source": [
    "### Markdown document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31215693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install markdown -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13ee689a-eb41-48cb-aa2a-7989361ab800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Setting the file path\n",
    "file_path_markdown = \"../data/README.md\"\n",
    "\n",
    "# Step 2: Create loader that is specific for your document type\n",
    "loader_markdown = UnstructuredMarkdownLoader(file_path_markdown)\n",
    "\n",
    "# Step 3: Load in the document\n",
    "my_markdown_document = loader_markdown.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dec293ee-4c8a-4d9a-ab9b-37955f51c1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/README.md'}, page_content='Release Notes\\n\\nCI\\n\\nPyPI - License\\n\\nPyPI - Downloads\\n\\nGitHub star chart\\n\\nOpen Issues\\n\\nOpen in Dev Containers\\n\\n\\n\\nTwitter\\n\\nCodSpeed Badge\\n\\n[!NOTE] Looking for the JS/TS library? Check out LangChain.js.\\n\\nLangChain is a framework for building LLM-powered applications. It helps you chain together interoperable components and third-party integrations to simplify AI application development — all while future-proofing decisions as the underlying technology evolves.\\n\\nbash pip install -U langchain\\n\\nTo learn more about LangChain, check out the docs. If you’re looking for more advanced customization or agent orchestration, check out LangGraph, our framework for building controllable agent workflows.\\n\\nWhy use LangChain?\\n\\nLangChain helps developers build applications powered by LLMs through a standard interface for models, embeddings, vector stores, and more.\\n\\nUse LangChain for: - Real-time data augmentation. Easily connect LLMs to diverse data sources and external / internal systems, drawing from LangChain’s vast library of integrations with model providers, tools, vector stores, retrievers, and more. - Model interoperability. Swap models in and out as your engineering team experiments to find the best choice for your application’s needs. As the industry frontier evolves, adapt quickly — LangChain’s abstractions keep you moving without losing momentum.\\n\\nLangChain’s ecosystem\\n\\nWhile the LangChain framework can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools when building LLM applications.\\n\\nTo improve your LLM application development, pair LangChain with:\\n\\nLangSmith - Helpful for agent evals and observability. Debug poor-performing LLM app runs, evaluate agent trajectories, gain visibility in production, and improve performance over time.\\n\\nLangGraph - Build agents that can reliably handle complex tasks with LangGraph, our low-level agent orchestration framework. LangGraph offers customizable architecture, long-term memory, and human-in-the-loop workflows — and is trusted in production by companies like LinkedIn, Uber, Klarna, and GitLab.\\n\\nLangGraph Platform - Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in LangGraph Studio.\\n\\nAdditional resources\\n\\nTutorials: Simple walkthroughs with guided examples on getting started with LangChain.\\n\\nHow-to Guides: Quick, actionable code snippets for topics such as tool calling, RAG use cases, and more.\\n\\nConceptual Guides: Explanations of key concepts behind the LangChain framework.\\n\\nAPI Reference: Detailed reference on navigating base packages and integrations for LangChain.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_markdown_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7571da7-f411-445e-8b5d-7506955ecbbd",
   "metadata": {},
   "source": [
    "## 🦜 Langchain provide ways of loading in all types of documents \n",
    "\n",
    "Here is a reference for loading documents that I didn't cover in the examples. When we begin to work on our group project these links can be useful to help you load all types of document types.\n",
    "\n",
    "#### Reference: [Document loaders](https://python.langchain.com/docs/how_to/#document-loaders)\n",
    "+ [How to: load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n",
    "+ [How to: load web pages](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "+ [How to: load CSV data](https://python.langchain.com/docs/how_to/document_loader_csv/)\n",
    "+ [How to: load HTML data](https://python.langchain.com/docs/how_to/document_loader_html/)\n",
    "+ [How to: load JSON data](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "+ [How to: load Markdown data](https://python.langchain.com/docs/how_to/document_loader_markdown/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbb36e8-2608-42c7-a102-05909e27524b",
   "metadata": {},
   "source": [
    "## 📂 Step 2: Loading documents from a folder\n",
    "\n",
    "**Scenario:** What if you have a folder full of documents and you want to load in all of the PDF documents you have at the same time?\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/image1.png\" width=\"25%\" height=\"25%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6bff0-6687-48f4-b3ad-921639d3609c",
   "metadata": {},
   "source": [
    "### 2.1: Let's say we want to load in all of the PDF files we have in a folder into a document loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "238f750a-1bba-44f4-8407-3cec785b7add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Using Langchain's DirectoryLoader set the path to your folder and use\n",
    "pdfs_file_path = \"../data/\"\n",
    "\n",
    "# Step 2: use Directory loader to load in all of the documents from a folder\n",
    "\n",
    "loader_pdfs = DirectoryLoader(pdfs_file_path,                # set your base file path\n",
    "                              glob=\"*.pdf\",                  # *.pdf means all files with the extension .pdf \n",
    "                              show_progress=True,            # progress bar\n",
    "                              use_multithreading=True,       # load from disk using multiple threads\n",
    "                              loader_cls=PyPDFLoader         # add the langchain PDF class\n",
    "                             )\n",
    "\n",
    "# Step 3: Load in the documents\n",
    "my_pdf_documents = loader_pdfs.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28243f5e-dae9-4608-9101-a766cc36c09a",
   "metadata": {},
   "source": [
    "#### Let's talk about what just happend.\n",
    "\n",
    "In this project - there is a folder called `data/`. That folder contains three different documents that end in a .pdf.\n",
    "    \n",
    "+ data/principles-for-navigating-big-debt-crises-by-ray-dalio.pdf\n",
    "+ data/react.pdf\n",
    "+ data/trends_in_deep_learning_nlp.pdf\n",
    "\n",
    "`DirectoryLoader` took all of the documents ending in .pdf, saved them to a list of `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55f3c384-09bc-4164-84bf-18353128b0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}, page_content='Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf_documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd343898-c4b2-4a8a-a675-1d4e02cabbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source of the text:   ../data/react.pdf\n",
      "Page:                 0\n"
     ]
    }
   ],
   "source": [
    "# We can also look at the details of the Document object\n",
    "print(\"Source of the text:  \" , my_pdf_documents[0].metadata[\"source\"])\n",
    "print(\"Page:                \" , my_pdf_documents[0].metadata[\"page\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831da732-47c0-4632-a855-4ae1b6a31704",
   "metadata": {},
   "source": [
    "#### What if you have a scenario where you want to add more data to the `Document` object? \n",
    "\n",
    "+ For example - your data is missing the authors name, but you know the author of the document and want to add it to use if for a later task - for example, as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc83d397-f075-4e15-b927-cbc86d867027",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pdf_documents[0].metadata[\"author\"] = \"Andronik\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7ea33f9-5713-486a-b49a-d3cd52a921ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andronik'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf_documents[0].metadata[\"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7eb7c97-bab4-407f-b96f-310a47a8db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': 'Andronik', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}, page_content='Published as a conference paper at ICLR 2023\\nREAC T: S YNERGIZING REASONING AND ACTING IN\\nLANGUAGE MODELS\\nShunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\\n1Department of Computer Science, Princeton University\\n2Google Research, Brain team\\n1{shunyuy,karthikn}@princeton.edu\\n2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\\nABSTRACT\\nWhile large language models (LLMs) have demonstrated impressive performance\\nacross tasks in language understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g. action\\nplan generation) have primarily been studied as separate topics. In this paper, we\\nexplore the use of LLMs to generate both reasoning traces and task-speciﬁc actions\\nin an interleaved manner, allowing for greater synergy between the two: reasoning\\ntraces help the model induce, track, and update action plans as well as handle\\nexceptions, while actions allow it to interface with and gather additional information\\nfrom external sources such as knowledge bases or environments. We apply our\\napproach, named ReAct, to a diverse set of language and decision making tasks\\nand demonstrate its effectiveness over state-of-the-art baselines in addition to\\nimproved human interpretability and trustworthiness. Concretely, on question\\nanswering (HotpotQA) and fact veriﬁcation (Fever), ReAct overcomes prevalent\\nissues of hallucination and error propagation in chain-of-thought reasoning by\\ninteracting with a simple Wikipedia API, and generating human-like task-solving\\ntrajectories that are more interpretable than baselines without reasoning traces.\\nFurthermore, on two interactive decision making benchmarks (ALFWorld and\\nWebShop), ReAct outperforms imitation and reinforcement learning methods by\\nan absolute success rate of 34% and 10% respectively, while being prompted with\\nonly one or two in-context examples.\\n1 I NTRODUCTION\\nA unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with\\nverbal reasoning (or inner speech, Alderson-Day & Fernyhough, 2015), which has been theorized to\\nplay an important role in human cognition for enabling self-regulation or strategization (Vygotsky,\\n1987; Luria, 1965; Fernyhough, 2010) and maintaining a working memory (Baddeley, 1992). Con-\\nsider the example of cooking up a dish in the kitchen. Between any two speciﬁc actions, we may\\nreason in language in order to track progress (“now that everything is cut, I should heat up the pot of\\nwater”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let\\nme use soy sauce and pepper instead”), and to realize when external information is needed (“how do\\nI prepare dough? Let me search on the Internet”). We may also act (open a cookbook to read the\\nrecipe, open the fridge, check ingredients) to support the reasoning and to answer questions (“What\\ndish can I make right now?”). This tight synergy between “acting” and “reasoning” allows humans\\nto learn new tasks quickly and perform robust decision making or reasoning, even under previously\\nunseen circumstances or facing information uncertainties.\\nRecent results have hinted at the possibility of combining verbal reasoning with interactive decision\\nmaking in autonomous systems. On one hand, properly prompted large language models (LLMs)\\nhave demonstrated emergent capabilities to carry out several steps of reasoning traces to derive\\n∗Work during Google internship. Projet page with code: https://react-lm.github.io/.\\n1\\narXiv:2210.03629v3  [cs.CL]  10 Mar 2023')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_pdf_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e100f7ac-bb58-408e-b4f9-62ca0406fa03",
   "metadata": {},
   "source": [
    "### 2.2: Let's do something similar but with HTML files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a9c17f0-09d7-4652-8ba2-af4161aa8cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  3.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Using Langchain's DirectoryLoader set the path to your folder and use\n",
    "html_file_path = \"../data/\"\n",
    "\n",
    "# Step 2: use Directory loader to load in all of the documents from a folder\n",
    "\n",
    "loader_html = DirectoryLoader(html_file_path,                       \n",
    "                              glob=\"*.html\",\n",
    "                              show_progress=True,\n",
    "                              use_multithreading=True,\n",
    "                              loader_cls=UnstructuredHTMLLoader              # the only thing that changes from the top example\n",
    "                             )\n",
    "\n",
    "# Step 3: Load in the documents\n",
    "my_html_documents = loader_html.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39468d5c-ceb8-4840-b3f2-0f819b6ec2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source of the text:   ../data/Is_It_Wrong_to_Remove_a_Card_From_Monopoly_The_New_York_Times.html\n"
     ]
    }
   ],
   "source": [
    "# Same applies as above example\n",
    "print(\"Source of the text:  \" , my_html_documents[0].metadata[\"source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a0efb9a-d047-4beb-a771-29aa2217510f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Magazine|Is It Wrong to Remove a Card From Monopoly?\\n\\nhttps://www.nytimes.com/2025/04/16/magazine/monopoly-games-children-ethics.html\\n\\nAdvertisement\\n\\nSKIP ADVERTISEMENT\\n\\nSubscriber-only Newsletter\\n\\nThe Ethicist\\n\\nIs It Wrong to Remove a Card From Monopoly?\\n\\nThe magazine’s Ethicist columnist on altering board games to teach children ethical behavior.\\n\\nKwame Anthony Appiah\\n\\nBy Kwame Anthony Appiah\\n\\nApril 16, 2025\\n\\nYou’re reading The Ethicist newsletter, for Times subscribers only. Advice on life’s trickiest situations and moral dilemmas from the philosopher Kwame Anthony Appiah.\\n\\nMy grandchildren love playing Monopoly. The board game has become a great way for me to interact with them, and also a great way for them to see capitalism in all its imperfect glory. The problem: One of the cards a player may draw when landing on Community Chest is “Bank Error in Your Favor. Collect $200.” Right when we first started playing the game together, I removed that card from the set. I did so because i'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_html_documents[0].page_content[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdf4db-52d0-4055-b73e-55e7f12ff6f8",
   "metadata": {},
   "source": [
    "## 🗂️ Step 3: Loading different documents from the same folder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0bf20d2c-b329-443a-b995-9d81f32d805e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.56s/it]\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.75it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 90.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Loading in the PDF, HTML and Markdown files\n",
    "file_path_data = \"../data/\"\n",
    "\n",
    "loader_pdfs = DirectoryLoader(file_path_data,              \n",
    "                              glob=\"*.pdf\",              \n",
    "                              show_progress=True,            \n",
    "                              use_multithreading=True,     \n",
    "                              loader_cls=PyPDFLoader)                   # on difference is the loader class\n",
    "\n",
    "\n",
    "loader_html = DirectoryLoader(file_path_data,                       \n",
    "                              glob=\"*.html\",\n",
    "                              show_progress=True,\n",
    "                              use_multithreading=True,\n",
    "                              loader_cls=UnstructuredHTMLLoader)\n",
    "\n",
    "\n",
    "loader_markdown = DirectoryLoader(file_path_data,                       \n",
    "                                  glob=\"*.md\",\n",
    "                                  show_progress=True,\n",
    "                                  use_multithreading=True,\n",
    "                                  loader_cls=UnstructuredMarkdownLoader)\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: User MergeDataLoader to combine all of the different loader types you have for your data\n",
    "my_loaders = MergedDataLoader(loaders=[loader_pdfs, loader_html, loader_markdown])\n",
    "\n",
    "# Step 3: Load all documents into a list of Document objects\n",
    "my_docs = my_loaders.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d19d4fc0-dae3-490d-889e-f43794fd39c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of my list of Documents:  548\n"
     ]
    }
   ],
   "source": [
    "print(\"The length of my list of Documents: \", len(my_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ff6330a-f8ca-4e93-a091-8bf0f2476ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n",
      "{'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-03-13T00:09:11+00:00', 'author': '', 'keywords': '', 'moddate': '2023-03-13T00:09:11+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../data/react.pdf', 'total_pages': 33, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "while i < 10:\n",
    "    \n",
    "    for doc in my_docs:\n",
    "        print(doc.metadata)\n",
    "        break\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2f3e85-34fc-4d1d-ba3f-62d90a0e5752",
   "metadata": {},
   "source": [
    "---\n",
    "# Section 2: RAG\n",
    "\n",
    "What is RAG and why is it important? Let's say that you are working at a company and your boss want you to create a chatbot that is able to answer questions about your business. For example, how much inventory do we have left in the parts department? What are the type of defects we had for Part A in the month of March? \n",
    "\n",
    "To answer your bosses questions using a chatbot your chatbot needs to be up to date on the information your business has. Plus, the data your business has is not on the internet!\n",
    "\n",
    "RAG is a way of adding extra knowledge to the LLM without needing to retrain the entire model, which is expensive, time consuming and requires expertise. What RAG provides us is a database of additional information which we can \"hookup\" to an LLM and use to answer questions.\n",
    "\n",
    "Here is a diagram and next we will talk about how it works.\n",
    "\n",
    "![rag](../assets/rag2.png)\n",
    "\n",
    "\n",
    "Three important parts to a RAG:\n",
    "1. **Document Splitting** - how we chop of the documents into smaller pieces.\n",
    "2. **Embeddings** - how we take those smaller pieces and put them into a database.\n",
    "3. **Retrieval** - how we retrieve pieces of our documents from the RAG\n",
    "\n",
    "\n",
    "If you want more details RAG. Here is a paper you can check out...\n",
    "+ [What is a RAG?](https://www.youtube.com/watch?v=T-D1OfcDW1M)\n",
    "+ [Searching for Best Practices in Retrieval-Augmented\n",
    "Generation](https://arxiv.org/pdf/2407.01219)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc9392-90bc-4565-a54a-c5251e22c574",
   "metadata": {},
   "source": [
    "## Step 1: Document Splitting and Embeddings\n",
    "\n",
    "Once we know the documents that we want to include in our database, we need to processes the documents.\n",
    "**Chunking** is a way of splitting your documents into smaller pieces so when you need to retrieve something from your database you don't collect the entire document. You just collect a piece of information that is most important for your users question.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/text_splitting.png\" width=\"\" height=\"\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a135c0f-8cc3-4a0c-ab24-2087dcdc6b0e",
   "metadata": {},
   "source": [
    "## There a many different ways of splitting documents, but lets take a look at the few most common.\n",
    "\n",
    "\n",
    "### Step 1.1: Recursive Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c4eca09-c82d-4650-8607-608f28ee543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b87dae1-b345-4665-a482-163c810dbc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05ed5844-09e8-409e-bbdb-dcf9668ebaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"Hello. My name is Andronik and I have a cat named Jinxy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "813eb57f-e0aa-45b1-b629-40b1086ae339",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_text(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4c8117d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello. My',\n",
       " 'My name',\n",
       " 'name is',\n",
       " 'Andronik',\n",
       " 'and I',\n",
       " 'I have a',\n",
       " 'a cat',\n",
       " 'cat named',\n",
       " 'Jinxy']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b4b50b23-b5d3-4504-bcd8-3a21c0139cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk length: 4, Text Split: Hell\n",
      "Chunk length: 4, Text Split: llo.\n",
      "Chunk length: 2, Text Split: My\n",
      "Chunk length: 3, Text Split: nam\n",
      "Chunk length: 3, Text Split: ame\n",
      "Chunk length: 2, Text Split: is\n",
      "Chunk length: 3, Text Split: And\n",
      "Chunk length: 4, Text Split: ndro\n",
      "Chunk length: 4, Text Split: roni\n",
      "Chunk length: 3, Text Split: nik\n",
      "Chunk length: 3, Text Split: and\n",
      "Chunk length: 1, Text Split: I\n",
      "Chunk length: 3, Text Split: hav\n",
      "Chunk length: 3, Text Split: ave\n",
      "Chunk length: 1, Text Split: a\n",
      "Chunk length: 3, Text Split: cat\n",
      "Chunk length: 3, Text Split: nam\n",
      "Chunk length: 4, Text Split: amed\n",
      "Chunk length: 3, Text Split: Jin\n",
      "Chunk length: 4, Text Split: inxy\n",
      "------------------------------------------------------------\n",
      "Chunk length: 6, Text Split: Hello.\n",
      "Chunk length: 7, Text Split: My name\n",
      "Chunk length: 2, Text Split: is\n",
      "Chunk length: 7, Text Split: Androni\n",
      "Chunk length: 5, Text Split: ronik\n",
      "Chunk length: 5, Text Split: and I\n",
      "Chunk length: 6, Text Split: I have\n",
      "Chunk length: 5, Text Split: a cat\n",
      "Chunk length: 5, Text Split: named\n",
      "Chunk length: 5, Text Split: Jinxy\n",
      "------------------------------------------------------------\n",
      "Chunk length: 14, Text Split: Hello. My name\n",
      "Chunk length: 10, Text Split: My name is\n",
      "Chunk length: 15, Text Split: is Andronik and\n",
      "Chunk length: 12, Text Split: and I have a\n",
      "Chunk length: 10, Text Split: have a cat\n",
      "Chunk length: 11, Text Split: a cat named\n",
      "Chunk length: 11, Text Split: named Jinxy\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in [4, 8, 16]:\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size=n,\n",
    "    chunk_overlap=n//2)\n",
    "\n",
    "    text = text_splitter.split_text(test_sentence)\n",
    "\n",
    "    for t in text:\n",
    "        print(f\"Chunk length: {len(t)}, Text Split: {t}\")\n",
    "\n",
    "    print(\"------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbde3fd-7ba8-4206-8489-082f83c9d610",
   "metadata": {},
   "source": [
    "### Step 1.2: Semantic Chunking or Chunking by meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f5613934-5953-4ad4-81e6-6ca5aba82325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading in the api keys to connect to OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ed696b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain-openai langchain-experimental -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "615df564-8aa2-410f-98df-f60501e024be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ebea1c05-f153-429d-bbc8-8d1a61ebeea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to OpenAI \n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_organization = os.environ.get(\"OPENAI_ORGANIZATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "640d7623-79b6-459d-891f-28205ebacd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,\n",
    "                          model=\"text-embeddings-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "686c0ac0-a2dd-43ec-aa48-fc5baceb62b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0a38e629-5b72-49bf-a031-cec8a8696c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"\"\"\n",
    "This raises the question: Can function inlining affect the output of a numerical program? \n",
    "I’m interested in both the programmer inlining a function manually in his editor and the compiler doing it for you during an optimization pass.\n",
    "It turns out that yes, there are multiple ways in which inlining can change results, and specifics depend on the interplay of your language spec, \n",
    "compiler and hardware. One of the big reasons why compilers do inlining is that it increases the scope for optimizations. Most optimization passes \n",
    "run on individual functions, so inlining gives the compiler more code to work with, and more opportunities to apply potentially result-changing optimizations.\n",
    "Here’s a concrete example that compiled with gcc -O3 -march=haswell produces different results depending on whether the function is inlined or not\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e45f9e5-88ab-4216-b179-6d8b5fd41774",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.split_text(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0b34cfdd-7cd7-4884-996b-b301d61a230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 2\n",
      "--------------------------------------------------------------------\n",
      "\n",
      "This raises the question: Can function inlining affect the output of a numerical program? I’m interested in both the programmer inlining a function manually in his editor and the compiler doing it for you during an optimization pass. It turns out that yes, there are multiple ways in which inlining can change results, and specifics depend on the interplay of your language spec, \n",
      "compiler and hardware. One of the big reasons why compilers do inlining is that it increases the scope for optimizations.\n",
      "--------------------------------------------------------------------\n",
      "--------------------------------------------------------------------\n",
      "Most optimization passes \n",
      "run on individual functions, so inlining gives the compiler more code to work with, and more opportunities to apply potentially result-changing optimizations. Here’s a concrete example that compiled with gcc -O3 -march=haswell produces different results depending on whether the function is inlined or not\n",
      "\n",
      "--------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of chunks:\", len(docs))\n",
    "for doc in docs:\n",
    "    print(\"--------------------------------------------------------------------\")\n",
    "    print(doc)\n",
    "    print(\"--------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906da9f5-7f4d-4e4f-a89f-5e167d4f2f6d",
   "metadata": {},
   "source": [
    "### What did the `OpenAIEmbeddings()` do?\n",
    "\n",
    "The basic idea is this...\n",
    "\n",
    "The embeddings model take a look at the text that we give it and decides to divide it into 2 pieces. Each piece we can think of as an \"idea\". `Chunk 1` represents idea 2 and `Chunk 2` represents idea 2.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "node1[Document]\n",
    "node2[\"Chunk 1\"]\n",
    "node3[\"Chunk 2\"]\n",
    "node1-->node2\n",
    "node1-->node3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f98c1-68e0-4699-8899-81c367a26dab",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f76b9-01ce-425b-ab3f-2ffd7409c609",
   "metadata": {},
   "source": [
    "## Step 2: Creating a Database\n",
    "\n",
    "+ Now that we have taken the documents and split them into smaller pieces, we need to store them in a database so we can use them later.\n",
    "\n",
    "![vdb](../assets/vdb.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ec69551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install langchain-chroma -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "74f849bd-8faa-4f53-9955-d5edc2bd30b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import inspect\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b11b7650-f3e9-40fc-ba0a-deaf837d6766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first initilize an embeddings model to use on your documents\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key,\n",
    "                              model=\"text-embedding-3-large\",\n",
    "                              chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8006dca-14b9-4b08-8768-587095ed2b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: First initilize the SemanticChunker\n",
    "text_splitter = SemanticChunker(embeddings,\n",
    "                                breakpoint_threshold_type=\"percentile\",\n",
    "                                breakpoint_threshold_amount=70.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "84bd0143-b91d-4db4-92e4-cbb46246b3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Split the documents into section. \n",
    "#   We are hoping each section contains a seperate idea.\n",
    "docs = text_splitter.split_documents(my_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "616b14af-424b-4ac4-841c-406b514ddd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2023\n",
      "REAC T: S YNERGIZING REASONING AND ACTING IN\n",
      "LANGUAGE MODELS\n",
      "Shunyu Yao∗*,1, Jeffrey Zhao2, Dian Yu2, Nan Du2, Izhak Shafran2, Karthik Narasimhan1, Yuan Cao2\n",
      "1Department of Computer Science, Princeton University\n",
      "2Google Research, Brain team\n",
      "1{shunyuy,karthikn}@princeton.edu\n",
      "2{jeffreyzhao,dianyu,dunan,izhak,yuancao}@google.com\n",
      "ABSTRACT\n",
      "While large language models (LLMs) have demonstrated impressive performance\n",
      "across tasks in language understanding and interactive decision making, their\n",
      "abilities for reasoning (e.g. chain-of-thought prompting) and acting (e.g.' metadata={'source': '../data/react.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    print(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a19034bd-517d-4a61-ba4e-d254fe7f6642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Length after chunking: 3883\n"
     ]
    }
   ],
   "source": [
    "print(\"New Length after chunking:\", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca245c8b-f451-4edd-a5e5-586a80b014b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Insert your documents into a database after turning your docs into numbers.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    collection_name=\"tumo_2025\",\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"../db/my_first_vdb\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f570823e-6dcb-4d3d-8008-dd02913c6601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='8cd4758a-5fde-4bb3-81c7-828374fca164', metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='25\\nTABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\\nbeen adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\\nLanguage Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\\nRecognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\\nTask Previous SOTA Previous\\nSOTA Results Baseline ELMo +\\nBaseline\\nIncrease\\n(Absolute/Relative)\\nSQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\\nSRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\\nNER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\\nSST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\\nTask BiLSTM+\\nELMo+Attn BERT\\nQNLI 79.9 91.1\\nSST-2 90.9 94.9\\nSTS-B 73.3 86.5\\nRTE 56.8 70.1\\nSQuAD 85.8 91.1\\nNER 92.2 92.8\\nTABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\\nﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\\nQA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\\nDeep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\\ndistributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\\nlearning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\\nunlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\\nfor some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\\nbe employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\\ndriven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\\nexpect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\\nwith) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\\nis enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\\nwill be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\\nfact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\\ndecision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\\nChomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\\nand doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\\n[1] E.'),\n",
       " Document(id='e98a992b-7d6c-49c0-97d2-7bc74db019b5', metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='25\\nTABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\\nbeen adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\\nLanguage Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\\nRecognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\\nTask Previous SOTA Previous\\nSOTA Results Baseline ELMo +\\nBaseline\\nIncrease\\n(Absolute/Relative)\\nSQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\\nSRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\\nNER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\\nSST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\\nTask BiLSTM+\\nELMo+Attn BERT\\nQNLI 79.9 91.1\\nSST-2 90.9 94.9\\nSTS-B 73.3 86.5\\nRTE 56.8 70.1\\nSQuAD 85.8 91.1\\nNER 92.2 92.8\\nTABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\\nﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\\nQA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\\nDeep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\\ndistributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\\nlearning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\\nunlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\\nfor some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\\nbe employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\\ndriven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\\nexpect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\\nwith) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\\nis enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\\nwill be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\\nfact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\\ndecision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\\nChomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\\nand doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\\n[1] E.'),\n",
       " Document(id='4a885871-2232-4479-a6ca-6f4a3a0ed414', metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='25\\nTABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\\nbeen adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\\nLanguage Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\\nRecognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\\nTask Previous SOTA Previous\\nSOTA Results Baseline ELMo +\\nBaseline\\nIncrease\\n(Absolute/Relative)\\nSQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\\nSRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\\nNER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\\nSST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\\nTask BiLSTM+\\nELMo+Attn BERT\\nQNLI 79.9 91.1\\nSST-2 90.9 94.9\\nSTS-B 73.3 86.5\\nRTE 56.8 70.1\\nSQuAD 85.8 91.1\\nNER 92.2 92.8\\nTABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\\nﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\\nQA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\\nDeep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\\ndistributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\\nlearning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\\nunlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\\nfor some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\\nbe employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\\ndriven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\\nexpect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\\nwith) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\\nis enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\\nwill be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\\nfact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\\ndecision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\\nChomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\\nand doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\\n[1] E.'),\n",
       " Document(id='d73d3287-4c8e-4054-a9c5-372d9add86cb', metadata={'page': 0, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning\\nmethods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models\\n(e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks\\nbased on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by\\nthe success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature\\nrepresentation learning.')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run a test and see what the most similar documents to the phrase \"Machine Learning\"\n",
    "vectorstore.similarity_search(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "51b5dbb9-576a-47db-85a6-dbdda513e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Langhchain as a retriever method that makes it easier to use the vectorbase \n",
    "#         by allowing it to be easier to integrate into our chain of functions. \n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f947297b-8dbc-4f86-9a70-0a4a8281c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_retrieved_docs = retriever.invoke(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "47300131-cfaa-4e25-8bcb-a683d661e94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many documents we retrieved:  5\n"
     ]
    }
   ],
   "source": [
    "print(\"How many documents we retrieved: \", len(my_retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1c54e5e1-4e4c-4ba7-a1cc-c564a191f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='25\n",
      "TABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\n",
      "been adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\n",
      "Language Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\n",
      "Recognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\n",
      "Task Previous SOTA Previous\n",
      "SOTA Results Baseline ELMo +\n",
      "Baseline\n",
      "Increase\n",
      "(Absolute/Relative)\n",
      "SQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\n",
      "SNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\n",
      "SRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\n",
      "Coref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\n",
      "NER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\n",
      "SST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\n",
      "Task BiLSTM+\n",
      "ELMo+Attn BERT\n",
      "QNLI 79.9 91.1\n",
      "SST-2 90.9 94.9\n",
      "STS-B 73.3 86.5\n",
      "RTE 56.8 70.1\n",
      "SQuAD 85.8 91.1\n",
      "NER 92.2 92.8\n",
      "TABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\n",
      "ﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\n",
      "QA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\n",
      "Deep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\n",
      "distributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\n",
      "learning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\n",
      "unlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\n",
      "for some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\n",
      "be employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\n",
      "driven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\n",
      "designs. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\n",
      "expect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\n",
      "with) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\n",
      "is enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\n",
      "will be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\n",
      "fact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\n",
      "decision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\n",
      "Chomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\n",
      "and doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\n",
      "[1] E.' metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}\n",
      "----------\n",
      "page_content='25\n",
      "TABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\n",
      "been adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\n",
      "Language Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\n",
      "Recognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\n",
      "Task Previous SOTA Previous\n",
      "SOTA Results Baseline ELMo +\n",
      "Baseline\n",
      "Increase\n",
      "(Absolute/Relative)\n",
      "SQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\n",
      "SNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\n",
      "SRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\n",
      "Coref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\n",
      "NER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\n",
      "SST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\n",
      "Task BiLSTM+\n",
      "ELMo+Attn BERT\n",
      "QNLI 79.9 91.1\n",
      "SST-2 90.9 94.9\n",
      "STS-B 73.3 86.5\n",
      "RTE 56.8 70.1\n",
      "SQuAD 85.8 91.1\n",
      "NER 92.2 92.8\n",
      "TABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\n",
      "ﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\n",
      "QA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\n",
      "Deep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\n",
      "distributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\n",
      "learning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\n",
      "unlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\n",
      "for some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\n",
      "be employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\n",
      "driven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\n",
      "designs. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\n",
      "expect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\n",
      "with) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\n",
      "is enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\n",
      "will be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\n",
      "fact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\n",
      "decision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\n",
      "Chomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\n",
      "and doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\n",
      "[1] E.' metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}\n",
      "----------\n",
      "page_content='25\n",
      "TABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\n",
      "been adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\n",
      "Language Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\n",
      "Recognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\n",
      "Task Previous SOTA Previous\n",
      "SOTA Results Baseline ELMo +\n",
      "Baseline\n",
      "Increase\n",
      "(Absolute/Relative)\n",
      "SQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\n",
      "SNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\n",
      "SRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\n",
      "Coref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\n",
      "NER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\n",
      "SST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\n",
      "Task BiLSTM+\n",
      "ELMo+Attn BERT\n",
      "QNLI 79.9 91.1\n",
      "SST-2 90.9 94.9\n",
      "STS-B 73.3 86.5\n",
      "RTE 56.8 70.1\n",
      "SQuAD 85.8 91.1\n",
      "NER 92.2 92.8\n",
      "TABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\n",
      "ﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\n",
      "QA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\n",
      "Deep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\n",
      "distributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\n",
      "learning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\n",
      "unlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\n",
      "for some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\n",
      "be employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\n",
      "driven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\n",
      "designs. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\n",
      "expect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\n",
      "with) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\n",
      "is enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\n",
      "will be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\n",
      "fact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\n",
      "decision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\n",
      "Chomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\n",
      "and doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\n",
      "[1] E.' metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}\n",
      "----------\n",
      "page_content='Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning\n",
      "methods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models\n",
      "(e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks\n",
      "based on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by\n",
      "the success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature\n",
      "representation learning.' metadata={'page': 0, 'source': '../data/trends_in_deep_learning_nlp.pdf'}\n",
      "----------\n",
      "page_content='Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning\n",
      "methods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models\n",
      "(e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks\n",
      "based on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by\n",
      "the success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature\n",
      "representation learning.' metadata={'page': 0, 'source': '../data/trends_in_deep_learning_nlp.pdf'}\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for doc in my_retrieved_docs:\n",
    "    print(doc)\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561c124-005b-4e6e-9706-563682393a44",
   "metadata": {},
   "source": [
    "#### How to load in your database after you have created it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b61c6af6-21a1-4021-be93-38ce12589842",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_vectorstore = Chroma(\n",
    "    persist_directory=\"../db/my_first_vdb\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"tumo_2025\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26f70c31-e31e-4c8a-a737-58b4b6b3f739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='8cd4758a-5fde-4bb3-81c7-828374fca164', metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='25\\nTABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\\nbeen adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\\nLanguage Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\\nRecognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\\nTask Previous SOTA Previous\\nSOTA Results Baseline ELMo +\\nBaseline\\nIncrease\\n(Absolute/Relative)\\nSQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\\nSRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\\nNER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\\nSST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\\nTask BiLSTM+\\nELMo+Attn BERT\\nQNLI 79.9 91.1\\nSST-2 90.9 94.9\\nSTS-B 73.3 86.5\\nRTE 56.8 70.1\\nSQuAD 85.8 91.1\\nNER 92.2 92.8\\nTABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\\nﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\\nQA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\\nDeep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\\ndistributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\\nlearning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\\nunlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\\nfor some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\\nbe employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\\ndriven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\\nexpect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\\nwith) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\\nis enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\\nwill be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\\nfact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\\ndecision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\\nChomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\\nand doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\\n[1] E.'),\n",
       "  1.1672812700271606),\n",
       " (Document(id='e98a992b-7d6c-49c0-97d2-7bc74db019b5', metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='25\\nTABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\\nbeen adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\\nLanguage Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\\nRecognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\\nTask Previous SOTA Previous\\nSOTA Results Baseline ELMo +\\nBaseline\\nIncrease\\n(Absolute/Relative)\\nSQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\\nSRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\\nNER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\\nSST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\\nTask BiLSTM+\\nELMo+Attn BERT\\nQNLI 79.9 91.1\\nSST-2 90.9 94.9\\nSTS-B 73.3 86.5\\nRTE 56.8 70.1\\nSQuAD 85.8 91.1\\nNER 92.2 92.8\\nTABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\\nﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\\nQA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\\nDeep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\\ndistributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\\nlearning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\\nunlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\\nfor some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\\nbe employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\\ndriven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\\nexpect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\\nwith) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\\nis enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\\nwill be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\\nfact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\\ndecision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\\nChomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\\nand doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\\n[1] E.'),\n",
       "  1.1711175441741943),\n",
       " (Document(id='4a885871-2232-4479-a6ca-6f4a3a0ed414', metadata={'page': 24, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='25\\nTABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\\nbeen adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\\nLanguage Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\\nRecognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\\nTask Previous SOTA Previous\\nSOTA Results Baseline ELMo +\\nBaseline\\nIncrease\\n(Absolute/Relative)\\nSQuAD Liu et al. [181] 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Qian et al. [182] 88.6 88.0 88.70 ±0.17 0.7 / 5.8%\\nSRL Luheng et al. [183] 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Kenton et al. [184] 67.2 67.2 70.4 3.2 / 9.8%\\nNER Matthew et al. [185] 91.93 ±0.19 90.15 92.22 ±0.10 2.06 / 21%\\nSST-5 Bryan et al. [186] 53.7 51.4 54.7 0.5 3.3 / 6.8%\\nTask BiLSTM+\\nELMo+Attn BERT\\nQNLI 79.9 91.1\\nSST-2 90.9 94.9\\nSTS-B 73.3 86.5\\nRTE 56.8 70.1\\nSQuAD 85.8 91.1\\nNER 92.2 92.8\\nTABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\\nﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\\nQA task; NER [180]: Named Entity Recognition. IX. C ONCLUSION\\nDeep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\\ndistributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\\nlearning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\\nunlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\\nfor some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\\nbe employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\\ndriven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\\ndesigns. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\\nexpect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\\nwith) other signals. Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\\nis enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\\nwill be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\\nfact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\\ndecision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\\nChomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\\nand doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”. REFERENCES\\n[1] E.'),\n",
       "  1.1715388298034668),\n",
       " (Document(id='d73d3287-4c8e-4054-a9c5-372d9add86cb', metadata={'page': 0, 'source': '../data/trends_in_deep_learning_nlp.pdf'}, page_content='Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning\\nmethods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models\\n(e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks\\nbased on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by\\nthe success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature\\nrepresentation learning.'),\n",
       "  1.1740833520889282)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_vectorstore.similarity_search_with_score(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf0c90",
   "metadata": {},
   "source": [
    "## Hands‑On Activity — End‑to‑End RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066b418",
   "metadata": {},
   "source": [
    "\n",
    "**Goal:**  \n",
    "1. Embed & store a small doc collection  \n",
    "2. Retrieve top‑k chunks for a user query  \n",
    "3. Feed those chunks into an LLM prompt and compare the answer quality\n",
    "\n",
    "We'll use:\n",
    "* `langchain` (or `langchain-community` ≥ 0.2)  \n",
    "* `sentence-transformers` or `openai` for embeddings  \n",
    "* `chromadb` for the vector index  \n",
    "* Your preferred LLM provider (OpenAI shown below)\n",
    "\n",
    "> **Tip:** If you're offline or behind a firewall, install dependencies with `pip install` before running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "926b1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qU langchain-community sentence-transformers faiss-cpu openai python-dotenv\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "\n",
    "# 1. Load sample docs (use your own folder path)\n",
    "docs = \"...\"\n",
    "\n",
    "# 2. Chunk\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=\"...\", \n",
    "                                          chunk_overlap=\"...\")\n",
    "\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "# 3. Embed \n",
    "embeddings = OpenAIEmbeddings()  # or HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# 4. To Database\n",
    "vectorstore = Chroma.from_documents(split_docs\n",
    "                                    documents=docs,\n",
    "                                    collection_name=\"tumo_2025\",\n",
    "                                    embedding=embeddings,\n",
    "                                    persist_directory=\"../db/my_second_vdb\")\n",
    "\n",
    "# 4. Retrieval\n",
    "user_query = \"Explain the mission statement of ACME Corp.\"\n",
    "top_k = 4\n",
    "retrieved_docs = vectordb.similarity_search(user_query, k=top_k)\n",
    "context = \"\\n\\n\".join([d.page_content for d in retrieved_docs])\n",
    "\n",
    "# 5. Build prompt & call LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = f\"Use the following context to answer the question.\\n\\nContext:\\n{context}\\n\\nQuestion: {user_query}\\nAnswer:\"\n",
    "\n",
    "answer = llm.invoke(prompt).content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b2194-8501-4f91-ad14-2831fdde2059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tumo_env) July 1",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
