{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Introduction to LLMs and Prompting\n",
    "\n",
    "\n",
    "**Summary:** In this notebook, we will cover how to connect to a Large Language Model and walkthrough different prompting techniques that will be useful during this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai      # Langchain OpenAI package\n",
    "# !pip install rich                  # Helps print doc string\n",
    "# !pip install python-dotenv         # Helps hide envionrment API Keys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # operating system \n",
    "from rich import inspect # pretty print doctring\n",
    "\n",
    "# Connect to OpenAI models\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv # load enviornment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to OpenAI \n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_organization = os.environ.get(\"OPENAI_ORGANIZATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Intro to Langchain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connecting to OpenAI\n",
    "\n",
    "First we need to connect to a large language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key = api_key, \n",
    "    openai_organization = organization,\n",
    "    model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-bf079f4a-a998-49a8-a0f7-cc590412dbde-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 2: Let's take a closer look at `ChatOpenAI`\n",
    "\n",
    "The API we will be using to connect to ChatGPT is called [LangChain](https://python.langchain.com/docs/introduction/).\n",
    "> LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "Langchain is a easy way of connecting to ChatGPT so we can start building application on top of ChatGPT. Throughout this workshop we will predominantly use this library to build our application.\n",
    "\n",
    "Now let's take a closer look at `ChatOpenAI`, which is how we connect to ChatGPT. In this section, I'll go through some of the most important parameters to set for this function, but here is the the documentation if you want to dive deeper into the API. \n",
    "+ Ref: https://sj-langchain.readthedocs.io/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "1. **openai_api_key** and **openai_organization** - will be provided to you, which you will need to log into your account. Think of this as user name and password.\n",
    "2. **model** - here we select the model that we want to use for our workshop\n",
    "3. **temperature** - adjust the creativity of your response. With a lower temperature the model is more conservative with it's reponse, usually resulting in shorter more concise answers. With a high temperature models become more creative and \"talk\" more. Usually, resulting in higher word count for a response\n",
    "4. **max_token** - maximum number of words you want to get back from the LLM.\n",
    "\n",
    "This is enough to get started, but eventually you might want to add more configurations. See [link](https://sj-langchain.readthedocs.io/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOpenAI(\n",
    "    openai_api_key = api_key, \n",
    "    openai_organization = organization,\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2.invoke(\"What is the capital of Armenia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Armenia is Yerevan.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'stop', 'logprobs': None}, id='run-915514de-f3fa-4261-8219-ef68f30aab96-0', usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let take a closer look at what Langchain's `ChatOpenAI` returns\n",
    "\n",
    "`AIMessage` is the object that is returned after we make an API call to GTP-4o-mini.\n",
    "We can access the message content, response metadata, the model we used to make the API call, a unique ID of the message we sent and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:      The capital of Armenia is Yerevan.\n",
      "Model name:  gpt-4o-mini-2024-07-18\n",
      "ID:          run-915514de-f3fa-4261-8219-ef68f30aab96-0\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:     \", response.content)\n",
    "print(\"Model name: \", response.response_metadata[\"model_name\"])\n",
    "print(\"ID:         \",response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Intro to Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is prompting?\n",
    "> Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). - Prompt Engineering Guide\n",
    "\n",
    "\n",
    "In this section, we are going to introduce more effective way's of putting toghther questions to send to ChatGPT. I'm going to go through some techniques that you have probably already used and some techniques that will be new to you, but will improve how well ChatGPT respones to your questions. \n",
    "\n",
    "\n",
    "**Summary:** Prompting techniques we will cover in this section.\n",
    "1. Zero-shot prompting\n",
    "2. Few-shot prompting\n",
    "3. Chain-of-Thought prompting\n",
    "4. Meta-prompting\n",
    "\n",
    "But first - let's look at how we can format prompts in Langchain\n",
    "\n",
    "What are `PromptTemplate`?\n",
    ">Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "\n",
    "**References**\n",
    "+ https://www.promptingguide.ai/\n",
    "+ Zero-shot/Few-shot paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)\n",
    "+ Chain-of-Thought paper: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)\n",
    "+ Meta-prompting paper: [Meta Prompting for AI Systems](https://arxiv.org/pdf/2311.11482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of \u001b[33;1m\u001b[1;3m{country}\u001b[0m?\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate - \n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"What is the capital of {country}?\"\n",
    ")\n",
    "\n",
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "content='The capital of Armenia is Yerevan.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_64e0ac9789', 'finish_reason': 'stop', 'logprobs': None} id='run-25b60e8a-7873-4370-aa6f-22c70741d237-0' usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "---------------------------\n",
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'stop', 'logprobs': None} id='run-7ca593bc-6b59-4dd5-b120-f1b0fc902db1-0' usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "---------------------------\n",
      "content='The capital of Germany is Berlin.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 14, 'total_tokens': 22, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None} id='run-1fe00448-9834-4124-b41a-72bd1d9e2218-0' usage_metadata={'input_tokens': 14, 'output_tokens': 8, 'total_tokens': 22, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | llm2\n",
    "\n",
    "print(\"---------------------------\")\n",
    "print(chain.invoke({\"country\": \"Armenia\"}))\n",
    "print(\"---------------------------\")\n",
    "print(chain.invoke({\"country\": \"France\"}))\n",
    "print(\"---------------------------\")\n",
    "print(chain.invoke({\"country\": \"Germany\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** My adding a simple prompt template we can reuse our prompt to ask the same question, but in difference ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 1: Zero-shot Prompting\n",
    "\n",
    "**Zero-shot prompting** is just asking your model a question. No context, no instructions, just asking a question to get the answer you want.\n",
    "\n",
    ">Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sentiment: Neutral', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 32, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None}, id='run-a82fc59b-6154-4da1-9748-588783892870-0', usage_metadata={'input_tokens': 32, 'output_tokens': 5, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"\"\"\n",
    "Classify the text into neutral, negative or positive. \n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2: Few-Shot Prompting\n",
    "\n",
    "**Few-Shot Prompting** enables your model to learn called \"in-context learning\" when you ask it a question.\n",
    ">Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What a horrible show! // Negative', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 37, 'total_tokens': 45, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_80cf447eee', 'finish_reason': 'stop', 'logprobs': None}, id='run-3c5d4ca1-8d8b-40ae-8f84-28445eef9037-0', usage_metadata={'input_tokens': 37, 'output_tokens': 8, 'total_tokens': 45, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"\"\"\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3: Chain-of-Thought Prompting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
