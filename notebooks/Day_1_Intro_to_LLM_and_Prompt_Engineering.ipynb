{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1: Introduction to Large Language Models (LLMs) and Prompt Engineering\n",
    "\n",
    "**Summary:** In this notebook we will explore the origins and evolution of large language models, understand how they are trained and the data that powers them, examine real‚Äëworld use‚Äëcases across industries, and finish with hands‚Äëon practice connecting to an LLM and crafting effective prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A Brief History of LLMs\n",
    "\n",
    "Large language models trace their lineage to early *n‚Äëgram* language models of the 1990s, through **word2vec** (2013) and **seq2seq** models (2014), to the breakthrough **Transformer** architecture introduced in 2017.  \n",
    "Key milestones:\n",
    "\n",
    "| Year | Milestone | Why it mattered |\n",
    "|------|-----------|-----------------|\n",
    "| 2017 | Attention Is All You Need | Introduced the Transformer, enabling parallel training and long‚Äërange context |\n",
    "| 2018 | GPT‚Äë1 | First generative pre‚Äëtrained Transformer; showed the power of pre‚Äëtraining + fine‚Äëtuning |\n",
    "| 2020 | GPT‚Äë3 (175‚ÄØB params) | Demonstrated emergent zero‚Äëshot and few‚Äëshot abilities at scale |\n",
    "| 2022 | Instruction‚Äëtuned (InstructGPT) | Reinforcement learning from human feedback makes models more helpful |\n",
    "| 2023‚Äë2025 | GPT‚Äë4, Claude‚Äë3, Gemini, open‚Äëweight 70‚ÄëB+ models | Multimodal input, tool use, and rapid ecosystem growth |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How LLMs Are Trained\n",
    "\n",
    "LLM training is typically split into two (sometimes three) phases:\n",
    "\n",
    "1. **Pre‚Äëtraining** ‚Äì self‚Äësupervised learning on trillions of tokens (web crawl, books, code, research papers). Objective: predict the next token.  \n",
    "2. **Instruction‚Äëtuning** ‚Äì supervised fine‚Äëtuning on curated *instruction¬†‚Üí¬†response* pairs (~500k‚Äë2M examples).  \n",
    "3. **Alignment (RLHF / DPO / RLAIF)** ‚Äì align model behavior with human preferences through reinforcement learning from human feedback or preference distillation.\n",
    "\n",
    "> **Compute formula (rough)**: *Training FLOPs* ‚âà 6‚ÄØ√ó‚ÄØ#parameters‚ÄØ√ó‚ÄØ#tokens. Scaling laws show loss ‚àù (compute)^‚ÄëŒ±.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What Data Do LLMs Use?\n",
    "\n",
    "| Data source | Typical share | Examples |\n",
    "|-------------|--------------|----------|\n",
    "| Common Crawl snapshots | 40‚Äë60‚ÄØ% | raw web pages, deduplicated & filtered |\n",
    "| Books | 10‚Äë20‚ÄØ% | Gutenberg, Books3, proprietary corpora |\n",
    "| Wikipedia | <3‚ÄØ% | English & multilingual dumps |\n",
    "| Scientific papers | 5‚Äë10‚ÄØ% | arXiv, PubMed |\n",
    "| Code | 5‚Äë10‚ÄØ% | GitHub, BigQuery GH datasets |\n",
    "| Dialog / examples | <1‚ÄØ% | ShareGPT, Stack Exchange, reddit |\n",
    "\n",
    "Modern pipelines include *deduplication, toxicity filtering, language balancing,* and *quality scores* (e.g., **CCNet** perplexity filtering).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Practical Applications in Industry\n",
    "\n",
    "| Sector | Use‚Äëcase | Impact |\n",
    "|--------|----------|--------|\n",
    "| Customer support | Automated chat, email triage | 24/7 availability, cost reduction |\n",
    "| Healthcare | Medical note summarization, patient Q&A (HIPAA compliant) | Clinician time savings |\n",
    "| Finance | Report generation, SEC filing Q&A, risk analysis | Faster insight discovery |\n",
    "| Legal | Contract analysis, clause extraction | Reduce review time |\n",
    "| Software dev | Code completion, refactoring, test generation | ‚Üë developer productivity |\n",
    "| Education | Adaptive tutoring, content generation | Personalized learning at scale |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Workshop Roadmap\n",
    "\n",
    "Over the next two weeks we will cover:\n",
    "\n",
    "1. **Day‚ÄØ1**¬†‚Äì History, training, data, applications, prompt engineering fundamentals  \n",
    "2. **Day‚ÄØ2**¬†‚Äì Retrieval‚ÄëAugmented Generation (RAG), Vector databases & embeddings    \n",
    "3. **Day‚ÄØ3**¬†‚Äì Agents\n",
    "4. **Day‚ÄØ4**¬†‚Äì Workflows  \n",
    "5. **Day‚ÄØ5**¬†‚Äì Multi‚ÄëAgent Systems (MAS)  \n",
    "6. **Week‚ÄØ2** ‚Äì Project work, advanced topics, deployment, capstone presentations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hands‚ÄëOn‚ÄØüöÄ¬†‚Äì Set Up Your Development Environment\n",
    "\n",
    "Follow the steps below **before** running any code cells:\n",
    "\n",
    "1. **Install dependencies**\n",
    "\n",
    "```bash\n",
    "pip install --upgrade langchain-openai python-dotenv rich\n",
    "```\n",
    "\n",
    "2. **Add your API credentials**\n",
    "\n",
    "Create a file named `.env` in the same directory with\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=\"...\"\n",
    "OPENAI_ORGANIZATION=\"...\"\n",
    "```\n",
    "\n",
    "3. **Reload the notebook kernel** so that environment variables are picked up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Activity¬†1¬†‚Äì Prompt Effect Experiment\n",
    "\n",
    "1. Run the baseline prompt:\n",
    "\n",
    "```python\n",
    "llm.invoke(\"Translate the following sentence to French: 'The weather is nice today.'\")\n",
    "```\n",
    "\n",
    "2. Now add context and style instructions:\n",
    "\n",
    "```python\n",
    "system_prompt = \"You are a poetic translator that prefers elegant, formal French.\"\n",
    "llm_poetic = llm.with_system_message(system_prompt)\n",
    "llm_poetic.invoke(\"Translate the following sentence to French: 'The weather is nice today.'\")\n",
    "```\n",
    "\n",
    "3. Compare outputs. Which version is more formal?  \n",
    "4. Try adjusting **temperature** and **max_tokens** in `ChatOpenAI` constructor and observe differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity¬†2¬†‚Äì Effect of Adding Context\n",
    "\n",
    "Create two calls:\n",
    "\n",
    "```python\n",
    "question = \"Who wrote 'Pride and Prejudice'?\"\n",
    "\n",
    "# Call 1: No context\n",
    "res_plain = llm.invoke(question)\n",
    "\n",
    "# Call 2: With context embedded in the prompt\n",
    "context = \"Answer in one short sentence.\"\n",
    "res_context = llm.invoke(f\"{context} {question}\")\n",
    "print(\"Plain:\", res_plain)\n",
    "print(\"With context:\", res_context)\n",
    "```\n",
    "\n",
    "> **Discussion:** How does the additional instruction change the answer length and style?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-openai      # Langchain OpenAI package\n",
    "# !pip install rich                  # Helps print doc string\n",
    "# !pip install python-dotenv         # Helps hide envionrment API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # operating system\n",
    "from rich import inspect # pretty print doctring\n",
    "\n",
    "# Connect to OpenAI models\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv # load enviornment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to OpenAI\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "openai_organization = os.environ.get(\"OPENAI_ORGANIZATION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Intro to Langchain\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Connecting to OpenAI\n",
    "\n",
    "First we need to connect to a large language model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    openai_api_key = openai_api_key, \n",
    "    openai_organization = openai_organization,\n",
    "    model = \"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 11, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'finish_reason': 'stop', 'logprobs': None}, id='run-12f48247-7384-4f43-ae54-e902f3cbdc34-0', usage_metadata={'input_tokens': 11, 'output_tokens': 10, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Step 2: Let's take a closer look at `ChatOpenAI`\n",
    "\n",
    "The API we will be using to connect to ChatGPT is called [LangChain](https://python.langchain.com/docs/introduction/).\n",
    "> LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "Langchain is a easy way of connecting to ChatGPT so we can start building application on top of ChatGPT. Throughout this workshop we will predominantly use this library to build our application.\n",
    "\n",
    "Now let's take a closer look at `ChatOpenAI`, which is how we connect to ChatGPT. In this section, I'll go through some of the most important parameters to set for this function, but here is the the documentation if you want to dive deeper into the API. \n",
    "+ Ref: https://sj-langchain.readthedocs.io/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html\n",
    "\n",
    "\n",
    "### Parameters\n",
    "\n",
    "1. **openai_api_key** and **openai_organization** - will be provided to you, which you will need to log into your account. Think of this as user name and password.\n",
    "2. **model** - here we select the model that we want to use for our workshop\n",
    "3. **temperature** - adjust the creativity of your response. With a lower temperature the model is more conservative with it's reponse, usually resulting in shorter more concise answers. With a high temperature models become more creative and \"talk\" more. Usually, resulting in higher word count for a response\n",
    "4. **max_token** - maximum number of words you want to get back from the LLM.\n",
    "\n",
    "This is enough to get started, but eventually you might want to add more configurations. See [link](https://sj-langchain.readthedocs.io/en/latest/chat_models/langchain.chat_models.openai.ChatOpenAI.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOpenAI(\n",
    "    openai_api_key = openai_api_key, \n",
    "    openai_organization = openai_organization,\n",
    "    model = \"gpt-4o-mini\",\n",
    "    temperature=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm2.invoke(\"What is the capital of Armenia?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of Armenia is Yerevan.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 14, 'total_tokens': 24, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f7d56a8a2c', 'finish_reason': 'stop', 'logprobs': None}, id='run-804d9bb2-5ddb-4484-89c6-e78a6bcc982c-0', usage_metadata={'input_tokens': 14, 'output_tokens': 10, 'total_tokens': 24, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let take a closer look at what Langchain's `ChatOpenAI` returns\n",
    "\n",
    "`AIMessage` is the object that is returned after we make an API call to GTP-4o-mini.\n",
    "We can access the message content, response metadata, the model we used to make the API call, a unique ID of the message we sent and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:      The capital of Armenia is Yerevan.\n",
      "Model name:  gpt-4o-mini-2024-07-18\n",
      "ID:          run-804d9bb2-5ddb-4484-89c6-e78a6bcc982c-0\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer:     \", response.content)\n",
    "print(\"Model name: \", response.response_metadata[\"model_name\"])\n",
    "print(\"ID:         \",response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Intro to Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is prompting?\n",
    "> Prompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics. Prompt engineering skills help to better understand the capabilities and limitations of large language models (LLMs). - Prompt Engineering Guide\n",
    "\n",
    "\n",
    "In this section, we are going to introduce more effective way's of putting toghther questions to send to a LLM. I'm going to go through some techniques that you have probably already used and some techniques that will be new to you, but will improve how well ChatGPT respones to your questions. \n",
    "\n",
    "\n",
    "**Summary:** Prompting techniques we will cover in this section.\n",
    "1. Zero-shot prompting\n",
    "2. Few-shot prompting\n",
    "3. Chain-of-Thought prompting\n",
    "4. Meta-prompting\n",
    "\n",
    "But first - let's look at how we can format prompts in Langchain\n",
    "\n",
    "What are `PromptTemplate`?\n",
    ">Prompt templates help to translate user input and parameters into instructions for a language model. This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
    "\n",
    "\n",
    "**References**\n",
    "+ https://www.promptingguide.ai/\n",
    "+ Zero-shot/Few-shot paper: [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165)\n",
    "+ Chain-of-Thought paper: [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903)\n",
    "+ Meta-prompting paper: [Meta Prompting for AI Systems](https://arxiv.org/pdf/2311.11482)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the capital of \u001b[33;1m\u001b[1;3m{country}\u001b[0m?\n"
     ]
    }
   ],
   "source": [
    "# PromptTemplate \n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"What is the capital of {country}?\"\n",
    ")\n",
    "\n",
    "prompt_template.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "content='The capital of Armenia is Yerevan.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 14, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None} id='run--dbba9cbf-9feb-4cb1-a93c-e448e4ae5d5c-0' usage_metadata={'input_tokens': 14, 'output_tokens': 9, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "---------------------------\n",
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None} id='run--fbe2f868-61e1-4b3f-8262-78235c128061-0' usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "---------------------------\n",
      "content='The capital of Germany is Berlin.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'finish_reason': 'stop', 'logprobs': None} id='run--cdfaacfc-5a7c-4c34-8773-3ec835d3d8db-0' usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt_template | llm2\n",
    "\n",
    "print(\"---------------------------\")\n",
    "print(chain.invoke({\"country\": \"Armenia\"}))\n",
    "print(\"---------------------------\")\n",
    "print(chain.invoke({\"country\": \"France\"}))\n",
    "print(\"---------------------------\")\n",
    "print(chain.invoke({\"country\": \"Germany\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** My adding a simple prompt template we can reuse our prompt to ask the same question, but in difference ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 1: Zero-shot Prompting\n",
    "\n",
    "**Zero-shot prompting** is just asking your model a question. No context, no instructions, just asking a question to get the answer you want.\n",
    "\n",
    ">Zero-shot prompting means that the prompt used to interact with the model won't contain examples or demonstrations. The zero-shot prompt directly instructs the model to perform a task without any additional examples to steer it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sentiment: Neutral'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"\"\"\n",
    "Classify the text into neutral, negative or positive. \n",
    "Text: I think the vacation is okay.\n",
    "Sentiment:\n",
    "\"\"\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 2: Few-Shot Prompting\n",
    "\n",
    "**Few-Shot Prompting** enables your model to learn called \"in-context learning\" when you ask it a question.\n",
    ">Few-shot prompting can be used as a technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What a horrible show! // Negative'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"\"\"\n",
    "This is awesome! // Negative\n",
    "This is bad! // Positive\n",
    "Wow that movie was rad! // Positive\n",
    "What a horrible show! //\n",
    "\"\"\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique 3: Chain-of-Thought Prompting\n",
    "\n",
    "\n",
    "This is a prompting technique where you provide an example of how you reason through a complex problem, so that the LLM knows you solved the problem and mimics this type of reasoning.\n",
    ">Introduced in Wei et al. (2022), chain-of-thought (CoT) prompting enables complex reasoning capabilities through intermediate reasoning steps. \n",
    "You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's identify the odd numbers in the group: 15, 5, 13, 7, and 1.\\n\\nNow, let's add them together:\\n\\n15 + 5 + 13 + 7 + 1 = 41\\n\\nSince 41 is an odd number, the statement is False. The odd numbers in this group do not add up to an even number.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm2.invoke(\"\"\"\n",
    "The odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.\n",
    "A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.\n",
    "\n",
    "The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. \n",
    "A:\n",
    "\"\"\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Section 2: Examples of Prompt Types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 1: Zero-Shot Prompting\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/zsp.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 2: Few-Shot Prompting\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/fsp.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 3: Few-Shot Prompting\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/cot.png\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type 4: Meta-Prompting\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../assets/metap.png\"/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
